[

["methods-centered-on-predicting-at-risk-students.html", "Chapter 6 Methods centered on predicting at-risk students 6.1 Decision Trees and Random Forests 6.2 Neural Networks", " Chapter 6 Methods centered on predicting at-risk students Over the last twenty years, there has an increasing amount of work in the applied social sciences that explore the use of what (Breiman and others 2001) refers to as “algorithmic modelling”“, as opposed to”data modelling“. He describes these as two cultures, the former being made up of mostly computer scientists, and the latter being made up statisticians (the methods therein are the ones explored in the previous chapter of this report). The key metric in such classical methods are goodness of fit, and such”explanatory“” modelling aims to find associative and causal relationships between predictors. Meanwhile, “algorithmic”“, or”predictive“” methods emphasize determining any function that maps input variables to output responses, with less regard for a probabilitic framework that allows for causation, focusing solely on emprirical precision(Shmueli and others 2010). In the recently published Handbok of Learning Analytics(Lang et al. 2017), published by the Society of Learning Analytics Research, (Bergner 2017) asserts that the researchers looking into educational data stand to gain from understanding the nuances of both methodologies, as previous work has shown the strengths and weakeness of either in this domain. The previous chapter explored how classical statistical models can be built and used to determine what are the factors that influence dropout. This is useful for policy makers and admninstrators who want to dedicate resources in the most strategic places. However this chapter will explore models whose inner workings are less interpretable, but whose primary objective is prediction/identification of at-risk students. This is useful in the context where college administration has some blanket intervention that it would like to apply, and we just want to ensure that the students most in need are reached. Despite the less clear interpratability of factors in these predictive models (as compared to the explanatory models in the previous chapter), we will stil explore methods to “open up the black box”, and determine which features are most important in achieving both accrate and sensitive prediction. 6.1 Decision Trees and Random Forests 6.2 Neural Networks Placeholder "]

["descriptive-statistics.html", "Chapter 4 Descriptive Statistics 4.1 Demographics across the colleges and major programs 4.2 Fraction of students who change colleges", " Chapter 4 Descriptive Statistics Here in we will describe - the data set - the methods by which we label students at risk - the distributions of at-risk students by - demographic indicators - registration record indicators 4.1 Demographics across the colleges and major programs Do students who drop out do so because of poor grades? What fraction of students are counted year after year as drop-outs and labeled as problems to be solved by the system while being exemplary students in terms of academic performance. Armed with this dataset, we can get the answer to that question. Let us begin by looking at the average grades of students who eventually dropped out compared to grades of students who haven’t. The following set of graphs will look at that comparison for 3 different semesters: the semester in which they dropped out (or graduated), the semester before that and the one before that. Let us see what the data says. ## ## Welch Two Sample t-test ## ## data: students_last_session$Average[which(students_last_session$status == and students_last_session$Average[which(students_last_session$status == &quot;grad&quot;)] and &quot;out&quot;)] ## t = 78.251, df = 7349.6, p-value &lt; 2.2e-16 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 20.36923 21.41600 ## sample estimates: ## mean of x mean of y ## 79.97370 59.08109 We can see that…. Let us now look at a semester by semester basis. Let us start by looking at the average grades of students on the semester during which they are either graduating or dropping-out. ## ## Welch Two Sample t-test ## ## data: c2[, .SD[.N], by = c(&quot;student_number&quot;)][status == &quot;out&quot;]$V1 and c2[, .SD[.N], by = c(&quot;student_number&quot;)][status == &quot;grad&quot;]$V1 ## t = -84.565, df = 7363.6, p-value &lt; 2.2e-16 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -28.74741 -27.44482 ## sample estimates: ## mean of x mean of y ## 51.08037 79.17648 We can see that the data says that… Let us now turn our attention to the semester before the one where they graduate or drop out. As we go back in time, we are expecting the difference between the two groups to diminish. ## ## Welch Two Sample t-test ## ## data: c2[, .SD[.N - 1], by = c(&quot;student_number&quot;)][status == &quot;out&quot;]$V1 and c2[, .SD[.N - 1], by = c(&quot;student_number&quot;)][status == &quot;grad&quot;]$V1 ## t = -57.484, df = 6274.5, p-value &lt; 2.2e-16 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -17.01062 -15.88867 ## sample estimates: ## mean of x mean of y ## 64.24363 80.69328 Finally, let us look at 2 semesters before they graduate or drop-out. We are again expecting the same trend. ## ## Welch Two Sample t-test ## ## data: c2[, .SD[.N - 2], by = c(&quot;student_number&quot;)][status == &quot;out&quot;]$V1 and c2[, .SD[.N - 2], by = c(&quot;student_number&quot;)][status == &quot;grad&quot;]$V1 ## t = -41.354, df = 4162.2, p-value &lt; 2.2e-16 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -12.11730 -11.02037 ## sample estimates: ## mean of x mean of y ## 68.90087 80.46971 Sweet. Now let’s look at… 4.2 Fraction of students who change colleges "]

]
